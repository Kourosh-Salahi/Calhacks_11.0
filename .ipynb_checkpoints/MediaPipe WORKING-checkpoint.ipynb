{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e2d637-7571-40d4-8dc6-d9177571c115",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install opencv-python mediapipe msvc-runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5cfd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04e2a2ee-27f2-4c55-8cea-b503fc3592ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8f5226-a390-4742-ac77-da7e2f25a0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing the Holistic Model from Mediapipe and\n",
    "# Initializing the Model\n",
    "mp_holistic = mp.solutions.holistic\n",
    "holistic_model = mp_holistic.Holistic(\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    " \n",
    "# Initializing the drawing utils for drawing the facial landmarks on image\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd256618-afdf-453d-adea-d927f64e1575",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1729390443.289649 3861039 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M3\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1729390443.334384 3861118 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1729390443.343948 3861118 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1729390443.345424 3861120 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1729390443.345461 3861116 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1729390443.345746 3861118 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1729390443.348053 3861118 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1729390443.350246 3861120 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1729390443.350459 3861116 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "2024-10-19 19:14:03.408 Python[75104:3861039] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n",
      "W0000 00:00:1729390446.775691 3861116 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n",
      "/Users/amywang/Library/Python/3.9/lib/python/site-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To determine the ASL letter, we need to analyze the 3D vectors of the hand joints. However, without a direct mapping of the vectors to ASL letters, we can make an educated guess based on the structure of the hand.\n",
      "\n",
      "After analyzing the data, I noticed that the thumb is extended and the other fingers are curled in, which is a common gesture for the letter \"Y\" in ASL.\n",
      "\n",
      "ASL Letter: Y\n",
      "Y\n",
      "To determine the ASL letter, we need to analyze the 3D vectors of each joint. However, without a clear understanding of the orientation and position of the hand in 3D space, it's challenging to provide an accurate answer.\n",
      "\n",
      "That being said, we can try to make an educated guess based on the provided data. Here's a simplified approach:\n",
      "\n",
      "1. Calculate the distance between the wrist and each finger tip. This can give us an idea of the finger extension.\n",
      "2. Calculate the angle between the fingers and the wrist. This can give us an idea of the finger orientation.\n",
      "\n",
      "After analyzing the data, I found that:\n",
      "\n",
      "* The thumb tip is relatively close to the wrist, indicating that the thumb is not extended.\n",
      "* The index, middle, and ring fingers have similar distances from the wrist, indicating that they are extended.\n",
      "* The pinky finger is slightly closer to the wrist than the other fingers, indicating that it might be slightly bent.\n",
      "\n",
      "Based on these observations, it's possible that the ASL letter is 'E'. However, please note that this is a rough estimate and may not be accurate.\n",
      "\n",
      "ASL Letter: 'E'\n",
      "YE\n",
      "To determine the ASL letter, we need to analyze the hand pose represented by the given points. However, without visualizing the points or having a machine learning model trained on ASL hand poses, it's challenging to accurately identify the letter.\n",
      "\n",
      "That being said, we can make an educated guess by looking at the overall shape and orientation of the hand. \n",
      "\n",
      "Based on the given points, here's a possible interpretation:\n",
      "\n",
      "* The wrist (0) is at a relatively neutral position.\n",
      "* The thumb (1-4) is extended and pointing downwards, which could indicate a \"T\" or \"D\" shape.\n",
      "* The index finger (5-8) is also extended and pointing downwards, which could reinforce the \"T\" or \"D\" shape.\n",
      "* The middle finger (9-12) is slightly bent, but still pointing downwards.\n",
      "* The ring finger (13-16) and pinky finger (17-20) are also bent, but not as much as the middle finger.\n",
      "\n",
      "Considering these observations, a possible guess is that the ASL letter is \"T\". However, please note that this is a rough estimate and may not be accurate without further analysis or visualization.\n",
      "\n",
      "ASL Letter: T\n",
      "YET\n",
      "To determine the ASL letter, we need to analyze the hand pose described by the 3D vectors. Since the actual implementation of this analysis is complex and requires machine learning or computer vision techniques, I'll provide a simplified approach based on the provided data.\n",
      "\n",
      "After analyzing the data, I found that the hand pose seems to resemble the ASL letter 'N'. However, without more context or a detailed analysis of the hand pose, it's difficult to provide a definitive answer.\n",
      "\n",
      "Here's a simple Python code snippet that you can use to visualize the hand pose and get a better understanding of the ASL letter:\n",
      "\n",
      "```python\n",
      "import matplotlib.pyplot as plt\n",
      "from mpl_toolkits.mplot3d import Axes3D\n",
      "import numpy as np\n",
      "\n",
      "data = [[(0, 0.19555070996284485, 0.9424470663070679, 4.55558847534121e-07), (1, 0.25866594910621643, 0.9312261939048767, -0.003941091243177652), (2, 0.3231963813304901, 0.8834055066108704, -0.015264032408595085), (3, 0.37703755497932434, 0.8859238624572754, -0.032345302402973175), (4, 0.4234622120857239, 0.8836085796356201, -0.050289053469896317), (5, 0.30886057019233704, 0.6911157369613647, 0.004130885470658541), (6, 0.3372407853603363, 0.5798369646072388, -0.017254747450351715), (7, 0.37016022205352783, 0.5699804425239563, -0.036241259425878525), (8, 0.39824262261390686, 0.5909740924835205, -0.04727005586028099), (9, 0.289082407951355, 0.6795008778572083, -0.010317258536815643), (10, 0.31919482350349426, 0.\n",
      "No ASL letter found in the response.\n",
      "To determine the ASL letter, we need to analyze the hand pose represented by the given 3D vectors. However, without a visual representation or more context, it's challenging to accurately determine the ASL letter.\n",
      "\n",
      "However, I can provide some general insights and possible approaches to solve this problem:\n",
      "\n",
      "1. **Hand pose analysis**: Each ASL letter has a unique hand pose. By analyzing the 3D vectors, you can identify the orientation and position of the hand, fingers, and thumb.\n",
      "2. **Finger extension and curl**: ASL letters often involve specific finger extensions and curls. By examining the 3D vectors, you can determine which fingers are extended, curled, or in a neutral position.\n",
      "3. **Thumb position**: The thumb plays a crucial role in ASL letters. By analyzing the 3D vector corresponding to the thumb, you can determine its position and orientation.\n",
      "\n",
      "To provide a more accurate answer, I would need more information or a visual representation of the hand pose. However, based on the given data, I can attempt to make an educated guess.\n",
      "\n",
      "After analyzing the data, I notice that the thumb is relatively extended, and the fingers are in a neutral position. The middle fingers (MIDDLE_FINGER_MCP, MIDDLE_FINGER_PIP, MIDDLE_FINGER_DIP, MIDDLE_FINGER_TIP) are slightly curled, while the index and pinky fingers are relatively straight.\n",
      "\n",
      "Based on this analysis, I'm going to take a guess that the ASL letter is 'X'. However, please note that this is a rough estimate and might not be accurate.\n",
      "\n",
      "ASL Letter: X\n",
      "YETX\n",
      "To determine the ASL letter, we'll need to analyze the hand configuration. Since we don't have a visual representation of the hand, we'll rely on the given joint positions. \n",
      "\n",
      "Based on the provided joint positions, we can make some observations:\n",
      "\n",
      "- The thumb is extended and pointing downwards (THUMB_TIP's y-coordinate is lower than THUMB_CMC's).\n",
      "- The index finger is extended and pointing downwards (INDEX_FINGER_TIP's y-coordinate is lower than INDEX_FINGER_MCP's).\n",
      "- The middle finger is also extended and pointing downwards (MIDDLE_FINGER_TIP's y-coordinate is lower than MIDDLE_FINGER_MCP's).\n",
      "- The ring finger is extended and pointing downwards (RING_FINGER_TIP's y-coordinate is lower than RING_FINGER_MCP's).\n",
      "- The pinky is extended and pointing downwards (PINKY_TIP's y-coordinate is lower than PINKY_MCP's).\n",
      "\n",
      "Considering these observations, the hand configuration seems to be an open, flat hand with all fingers extended and pointing downwards. This configuration corresponds to the ASL letter 'A' or '5'. However, since the thumb is extended and pointing downwards, it's more likely to be the ASL letter '5'.\n",
      "\n",
      "ASL Letter: '5'\n",
      "No ASL letter found in the response.\n",
      "To determine the ASL letter, we need to analyze the hand posture and finger positions. Based on the provided data, we can make some observations:\n",
      "\n",
      "- The thumb is extended and pointing downwards (THUMB_TIP is below THUMB_CMC).\n",
      "- The index finger is bent and pointing downwards (INDEX_FINGER_TIP is below INDEX_FINGER_MCP).\n",
      "- The middle finger is bent and pointing downwards (MIDDLE_FINGER_TIP is below MIDDLE_FINGER_MCP).\n",
      "- The ring finger is bent and pointing downwards (RING_FINGER_TIP is below RING_FINGER_MCP).\n",
      "- The pinky finger is bent and pointing downwards (PINKY_TIP is below PINKY_MCP).\n",
      "\n",
      "Considering these observations, it appears that the hand is in a fist-like position with all fingers bent and the thumb extended downwards. This is a characteristic of the ASL letter 'A' or 'S' but more closely resembles 'A'.\n",
      "\n",
      "However, to accurately determine the letter, we would need more information or a more detailed analysis of the finger positions and hand posture.\n",
      "\n",
      "Based on the available data, it appears to be the ASL letter 'A'. \n",
      "\n",
      "ASL Letter: A\n",
      "YETXA\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 111\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    109\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError with API response: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 111\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# Enter key 'q' to break the loop\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cv2\u001b[38;5;241m.\u001b[39mwaitKey(\u001b[38;5;241m5\u001b[39m) \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m0xFF\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#WORKING LETTER CONCATENATION THING KINDVE IDK\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import re\n",
    "from groq import Groq\n",
    "import time\n",
    "\n",
    "# Initialize MediaPipe components\n",
    "mp_holistic = mp.solutions.holistic\n",
    "\n",
    "# Create a holistic model instance\n",
    "holistic_model = mp_holistic.Holistic()\n",
    "\n",
    "# Video capture from the default camera\n",
    "capture = cv2.VideoCapture(0)\n",
    "\n",
    "# Define frame-skip rate (e.g., process 1 out of every 'n' frames)\n",
    "frame_skip = 2\n",
    "frame_count = 0\n",
    "\n",
    "# Initialize API client\n",
    "GROQ_API_KEY = \"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\"\n",
    "client = Groq(api_key=GROQ_API_KEY)\n",
    "\n",
    "# Variable to store ASL letters\n",
    "result = \"\"\n",
    "\n",
    "while capture.isOpened():\n",
    "    ret, frame = capture.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Increment the frame count\n",
    "    frame_count += 1\n",
    "\n",
    "    # Skip frames based on the defined frame_skip rate\n",
    "    if frame_count % frame_skip != 0:\n",
    "        continue\n",
    "\n",
    "    # Converting the frame from BGR to RGB\n",
    "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Making predictions using holistic model\n",
    "    image.flags.writeable = False\n",
    "    results = holistic_model.process(image)\n",
    "    image.flags.writeable = True\n",
    "\n",
    "    hand_joints = []\n",
    "\n",
    "    # Extracting right hand landmarks if available\n",
    "    if results.right_hand_landmarks:\n",
    "        right_hand_joints = [\n",
    "            (id, lm.x, lm.y, lm.z)\n",
    "            for id, lm in enumerate(results.right_hand_landmarks.landmark)\n",
    "        ]\n",
    "        hand_joints.append(right_hand_joints)\n",
    "\n",
    "    # Extracting left hand landmarks if available\n",
    "    if results.left_hand_landmarks:\n",
    "        left_hand_joints = [\n",
    "            (id, lm.x, lm.y, lm.z)\n",
    "            for id, lm in enumerate(results.left_hand_landmarks.landmark)\n",
    "        ]\n",
    "        hand_joints.append(left_hand_joints)\n",
    "\n",
    "    # Prepare content for the API request\n",
    "    if hand_joints:\n",
    "        content = (\n",
    "            f\"{hand_joints} Each of these points is a number followed by a 3D vector \"\n",
    "            f\"(number, x, y, z), each of the numbers corresponds to a joint on the hand. \"\n",
    "            f\"0. WRIST 1. THUMB_CMC 2. THUMB_MCP 3. THUMB_IP 4. THUMB_TIP \"\n",
    "            f\"5. INDEX_FINGER_MCP 6. INDEX_FINGER_PIP 7. INDEX_FINGER_DIP 8. INDEX_FINGER_TIP \"\n",
    "            f\"9. MIDDLE_FINGER_MCP 10. MIDDLE_FINGER_PIP 11. MIDDLE_FINGER_DIP 12. MIDDLE_FINGER_TIP \"\n",
    "            f\"13. RING_FINGER_MCP 14. RING_FINGER_PIP 15. RING_FINGER_DIP 16. RING_FINGER_TIP \"\n",
    "            f\"17. PINKY_MCP 18. PINKY_PIP 19. PINKY_DIP 20. PINKY_TIP. \"\n",
    "            f\"What ASL letter does this create? Please return the answer in the format 'ASL Letter: 'X''.\"\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            completion = client.chat.completions.create(\n",
    "                model=\"llama-3.2-90b-vision-preview\",\n",
    "                messages=[{\"role\": \"user\", \"content\": content}],\n",
    "                temperature=0.5,\n",
    "                max_tokens=500,\n",
    "                top_p=1,\n",
    "                stream=False,\n",
    "            )\n",
    "\n",
    "            text = completion.choices[0].message.content\n",
    "            #print(text)\n",
    "\n",
    "            # Extract ASL letter from the API response\n",
    "            # ASL_letter_match = re.search(r'ASL Letter: [\"\\']([A-Z])[\"\\']', text)\n",
    "            # if ASL_letter_match:\n",
    "            #     asl_letter = ASL_letter_match.group(1)\n",
    "            #     result += asl_letter\n",
    "            #     print(result)  # Print the updated result\n",
    "\n",
    "            ASL_letter_match = re.search(r'ASL Letter:\\s*[\"\\']?([A-Z])[\"\\']?', text)\n",
    "\n",
    "            if ASL_letter_match:\n",
    "                asl_letter = ASL_letter_match.group(1)\n",
    "                result += asl_letter\n",
    "                print(result)\n",
    "            else:\n",
    "                print(\"No ASL letter found in the response.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error with API response: {e}\")\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Enter key 'q' to break the loop\n",
    "    if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the capture and destroy windows\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a34f0dc4-ecd5-4458-9c19-8504f38bed74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1729388910.844576 3843250 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M3\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1729388910.890325 3843345 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1729388910.899232 3843346 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1729388910.900816 3843343 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1729388910.900871 3843344 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1729388910.900877 3843349 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1729388910.903266 3843344 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1729388910.905442 3843343 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1729388910.905702 3843349 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "2024-10-19 18:48:30.960 Python[74887:3843250] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n",
      "W0000 00:00:1729388912.300956 3843346 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n",
      "/Users/amywang/Library/Python/3.9/lib/python/site-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPStatusError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/groq/_base_client.py:1018\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1018\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m err:  \u001b[38;5;66;03m# thrown on 4xx and 5xx status code\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/httpx/_models.py:763\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m message \u001b[38;5;241m=\u001b[39m message\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m, error_type\u001b[38;5;241m=\u001b[39merror_type)\n\u001b[0;32m--> 763\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request\u001b[38;5;241m=\u001b[39mrequest, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPStatusError\u001b[0m: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 69\u001b[0m\n\u001b[1;32m     67\u001b[0m GROQ_API_KEY \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgsk_Mbm9hNuZSZn5K2M95XULWGdyb3FYKujTTH8H6j2TtcVkmcMoRlMw\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     68\u001b[0m client \u001b[38;5;241m=\u001b[39m Groq(api_key\u001b[38;5;241m=\u001b[39mGROQ_API_KEY)\n\u001b[0;32m---> 69\u001b[0m completion \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mllama-3.2-90b-vision-preview\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m#print(completion.choices[0].message)\u001b[39;00m\n\u001b[1;32m     85\u001b[0m text \u001b[38;5;241m=\u001b[39m completion\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/groq/resources/chat/completions.py:287\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    175\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    176\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    177\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m    Creates a model response for the given chat conversation.\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;124;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/openai/v1/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/groq/_base_client.py:1244\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1230\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1231\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1232\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1239\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1240\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1241\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1242\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1243\u001b[0m     )\n\u001b[0;32m-> 1244\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/groq/_base_client.py:936\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    928\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    929\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    934\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    935\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 936\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/groq/_base_client.py:1024\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1023\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/groq/_base_client.py:1071\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1067\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying request to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m, options\u001b[38;5;241m.\u001b[39murl, timeout)\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m-> 1071\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1073\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1074\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1075\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1078\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1079\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # (joint, x, y, z) with frame-skip\n",
    "\n",
    "# import cv2\n",
    "# import mediapipe as mp\n",
    "# from groq import Groq\n",
    "# import re\n",
    "\n",
    "# # Initialize MediaPipe components\n",
    "# mp_holistic = mp.solutions.holistic\n",
    "# mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# # Create a holistic model instance\n",
    "# holistic_model = mp_holistic.Holistic()\n",
    "\n",
    "# # (0) in VideoCapture is used to connect to your computer's default camera\n",
    "# capture = cv2.VideoCapture(0)\n",
    "\n",
    "# # Define frame-skip rate (e.g., process 1 out of every 'n' frames)\n",
    "# frame_skip = 5\n",
    "# frame_count = 0\n",
    "\n",
    "# result = \"\"\n",
    "\n",
    "# while capture.isOpened():\n",
    "#     # Capture frame by frame\n",
    "#     ret, frame = capture.read()\n",
    "\n",
    "#     if not ret:\n",
    "#         break\n",
    "\n",
    "#     # Increment the frame count\n",
    "#     frame_count += 1\n",
    "\n",
    "#     # Skip frames based on the defined frame_skip rate\n",
    "#     if frame_count % frame_skip != 0:\n",
    "#         continue\n",
    "\n",
    "#     # Converting the frame from BGR to RGB\n",
    "#     image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "#     # Making predictions using holistic model\n",
    "#     image.flags.writeable = False\n",
    "#     results = holistic_model.process(image)\n",
    "#     image.flags.writeable = True\n",
    "\n",
    "#     hand_joints = []\n",
    "\n",
    "#     # Extracting right hand landmarks if available\n",
    "#     if results.right_hand_landmarks:\n",
    "#         right_hand_joints = []\n",
    "#         for id, lm in enumerate(results.right_hand_landmarks.landmark):\n",
    "#             right_hand_joints.append((id, lm.x, lm.y, lm.z))  # Store the landmark ID and normalized coordinates\n",
    "#         hand_joints.append(right_hand_joints)\n",
    "#         #print(\"Right Hand Joints:\", right_hand_joints)\n",
    "\n",
    "#     # Extracting left hand landmarks if available\n",
    "#     if results.left_hand_landmarks:\n",
    "#         left_hand_joints = []\n",
    "#         for id, lm in enumerate(results.left_hand_landmarks.landmark):\n",
    "#             left_hand_joints.append((id, lm.x, lm.y, lm.z))  # Store the landmark ID and normalized coordinates\n",
    "#         hand_joints.append(left_hand_joints)\n",
    "#         #print(\"Left Hand Joints:\", left_hand_joints)\n",
    "\n",
    "#     content = str(hand_joints) + \"Each of these points is a number followed by a 3 dimensional vector in space (number, x, y, z), each of the numbers corresponds to a joint on the hand. 0. WRIST 1. THUMB_CMC 2. THUMB_MCP  3. THUMB_IP 4. THUMB_TIP 5. INDEX FINGER_MCP 6. INDEX FINGER PIP 7. INDEX FINGER_DIP 8. INDEX FINGER TIP 9. MIDDLE FINGER_MCP 10. MIDDLE FINGER_PIP 11. MIDDLE FINGER DIP 12. MIDDLE FINGER TIP 13. RING FINGER MCP 14. RING FINGER PIP 15. RING FINGER_DIP 16. RING FINGER TIP 17. PINKY MCP 18. PINKY PIP 19. PINKY DIP 20. PINKY TIP What ASL letter does this create. Please return the answer in the format 'ASL Letter: 'X''.\"\n",
    "    \n",
    "#     if len(hand_joints) > 0: \n",
    "#         GROQ_API_KEY = \"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\"\n",
    "#         client = Groq(api_key=GROQ_API_KEY)\n",
    "#         completion = client.chat.completions.create(\n",
    "#             model=\"llama-3.2-90b-vision-preview\",\n",
    "#             messages=[\n",
    "#                 {\n",
    "#                     \"role\": \"user\",\n",
    "#                     \"content\": content\n",
    "#                 }\n",
    "#             ],\n",
    "#             temperature=1,\n",
    "#             max_tokens=1024,\n",
    "#             top_p=1,\n",
    "#             stream=False,\n",
    "#             stop=None,\n",
    "#         )\n",
    "#         #print(completion.choices[0].message)\n",
    "        \n",
    "#         text = completion.choices[0].message.content\n",
    "#         #print(text + \"\\n\")\n",
    "        \n",
    "#         ASL_letter = re.search(r'ASL Letter: [\"\\']([A-Z])[\"\\']', text)\n",
    "#         if ASL_letter:\n",
    "#             asl_letter = ASL_letter.group(1)\n",
    "#             print(asl_letter)\n",
    "    \n",
    "#         # if len(capital_letters) == 1: \n",
    "#         #     final_letters += capital_letters\n",
    "#         # elif len(capital_letters) > 1:\n",
    "#         #     unique_letters = []\n",
    "#         #     for letter in capital_letters:\n",
    "#         #         if letter.isalpha() and letter not in unique_letters:\n",
    "#         #                 unique_letters.append(letter)\n",
    "#         #     final_letters += unique_letters\n",
    "\n",
    "#         # if ASL_letter:\n",
    "#         #     result = result + ASL_letter\n",
    "#         #     print(result)\n",
    "        \n",
    "#         hand_joints.clear()\n",
    "\n",
    "#     # Enter key 'q' to break the loop\n",
    "#     if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "#         break\n",
    "\n",
    "# # When all the process is done\n",
    "# # Release the capture\n",
    "# capture.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3363dc-e88d-414f-8c38-6db5d64a60c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # (joint, x, y, z)\n",
    "# import cv2\n",
    "# import time\n",
    "# import mediapipe as mp\n",
    "# from groq import Groq\n",
    "\n",
    "# # Initialize MediaPipe components\n",
    "# mp_holistic = mp.solutions.holistic\n",
    "# mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# # Create a holistic model instance\n",
    "# holistic_model = mp_holistic.Holistic()\n",
    "\n",
    "# # (0) in VideoCapture is used to connect to your computer's default camera\n",
    "# capture = cv2.VideoCapture(0)\n",
    "\n",
    "# while capture.isOpened():\n",
    "#     # Capture frame by frame\n",
    "#     ret, frame = capture.read()\n",
    "\n",
    "#     # Converting the frame from BGR to RGB\n",
    "#     image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "#     # Making predictions using holistic model\n",
    "#     image.flags.writeable = False\n",
    "#     results = holistic_model.process(image)\n",
    "#     image.flags.writeable = True\n",
    "\n",
    "#     hand_joints = []\n",
    "\n",
    "#     # Extracting right hand landmarks if available\n",
    "#     if results.right_hand_landmarks:\n",
    "#         right_hand_joints = []\n",
    "#         for id, lm in enumerate(results.right_hand_landmarks.landmark):\n",
    "#             right_hand_joints.append((id, lm.x, lm.y, lm.z))  # Store the landmark ID and normalized coordinates\n",
    "#         hand_joints.append(right_hand_joints)\n",
    "#         #print(\"Right Hand Joints:\", right_hand_joints)\n",
    "\n",
    "#     # Extracting left hand landmarks if available\n",
    "#     if results.left_hand_landmarks:\n",
    "#         left_hand_joints = []\n",
    "#         for id, lm in enumerate(results.left_hand_landmarks.landmark):\n",
    "#             left_hand_joints.append((id, lm.x, lm.y, lm.z))  # Store the landmark ID and normalized coordinates\n",
    "#         hand_joints.append(left_hand_joints)\n",
    "#         #print(\"Left Hand Joints:\", left_hand_joints)\n",
    "    \n",
    "#     # Enter key 'q' to break the loop\n",
    "#     if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "#         break\n",
    "\n",
    "# # When all the process is done\n",
    "# # Release the capture\n",
    "# capture.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880c0e4a-f435-4af4-ade4-213ce2c55f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from groq import Groq\n",
    "\n",
    "#     content = str(hand_joints) + \"\\n\\neach of these points is a number followed by a 3 dimensional vector in space (number, x, y, z), each of the numbers corresponds to a joint on the hand. \\n\\n0. WRIST 1. THUMB_CMC 2. THUMB_MCP  3. THUMB_IP 4. THUMB_TIP 5. INDEX FINGER_MCP 6. INDEX FINGER PIP 7. INDEX FINGER_DIP 8. INDEX FINGER TIP 9. MIDDLE FINGER_MCP 10. MIDDLE FINGER_PIP 11. MIDDLE FINGER DIP 12. MIDDLE FINGER TIP 13. RING FINGER MCP 14. RING FINGER PIP 15. RING FINGER_DIP 16. RING FINGER TIP 17. PINKY MCP 18. PINKY PIP 19. PINKY DIP 20. PINKY TIP\\n\\nwhat ASL letter does this create. Please return just the letter.\"\n",
    "\n",
    "#     GROQ_API_KEY = \"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\"\n",
    "#     client = Groq(api_key=GROQ_API_KEY)\n",
    "#     completion = client.chat.completions.create(\n",
    "#         model=\"llama-3.2-90b-vision-preview\",\n",
    "#         messages=[\n",
    "#             {\n",
    "#                 \"role\": \"user\",\n",
    "#                 \"content\": content\n",
    "#             }\n",
    "#         ],\n",
    "#         temperature=1,\n",
    "#         max_tokens=1024,\n",
    "#         top_p=1,\n",
    "#         stream=False,\n",
    "#         stop=None,\n",
    "#     )\n",
    "#     print(completion.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b224b585-6d68-4738-91da-27281a9ee900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # (0) in VideoCapture is used to connect to your computer's default camera\n",
    "# capture = cv2.VideoCapture(0)\n",
    "\n",
    "# # Initializing current time and precious time for calculating the FPS\n",
    "# previousTime = 0\n",
    "# currentTime = 0\n",
    "\n",
    "# while capture.isOpened():\n",
    "# \t# capture frame by frame\n",
    "# \tret, frame = capture.read()\n",
    "\n",
    "# \t# resizing the frame for better view\n",
    "# \tframe = cv2.resize(frame, (800, 600))\n",
    "\n",
    "# \t# Converting the from BGR to RGB\n",
    "# \timage = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# \t# Making predictions using holistic model\n",
    "# \t# To improve performance, optionally mark the image as not writeable to\n",
    "# \t# pass by reference.\n",
    "# \timage.flags.writeable = False\n",
    "# \tresults = holistic_model.process(image)\n",
    "# \timage.flags.writeable = True\n",
    "\n",
    "# \t# Converting back the RGB image to BGR\n",
    "# \timage = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# \t# Drawing the Facial Landmarks\n",
    "# \tmp_drawing.draw_landmarks(\n",
    "# \timage,\n",
    "# \tresults.face_landmarks,\n",
    "# \tmp_holistic.FACEMESH_CONTOURS,\n",
    "# \tmp_drawing.DrawingSpec(\n",
    "# \t\tcolor=(255,0,255),\n",
    "# \t\tthickness=1,\n",
    "# \t\tcircle_radius=1\n",
    "# \t),\n",
    "# \tmp_drawing.DrawingSpec(\n",
    "# \t\tcolor=(0,255,255),\n",
    "# \t\tthickness=1,\n",
    "# \t\tcircle_radius=1\n",
    "# \t)\n",
    "# \t)\n",
    "\n",
    "# \t# Drawing Right hand Land Marks\n",
    "# \tmp_drawing.draw_landmarks(\n",
    "# \timage, \n",
    "# \tresults.right_hand_landmarks, \n",
    "# \tmp_holistic.HAND_CONNECTIONS\n",
    "# \t)\n",
    "\n",
    "# \t# Drawing Left hand Land Marks\n",
    "# \tmp_drawing.draw_landmarks(\n",
    "# \timage, \n",
    "# \tresults.left_hand_landmarks, \n",
    "# \tmp_holistic.HAND_CONNECTIONS\n",
    "# \t)\n",
    "\t\n",
    "# \t# Calculating the FPS\n",
    "# \tcurrentTime = time.time()\n",
    "# \tfps = 1 / (currentTime-previousTime)\n",
    "# \tpreviousTime = currentTime\n",
    "\t\n",
    "# \t# Displaying FPS on the image\n",
    "# \tcv2.putText(image, str(int(fps))+\" FPS\", (10, 70), cv2.FONT_HERSHEY_COMPLEX, 1, (0,255,0), 2)\n",
    "\n",
    "# \t# Display the resulting image\n",
    "# \tcv2.imshow(\"Facial and Hand Landmarks\", image)\n",
    "\n",
    "# \t# Enter key 'q' to break the loop\n",
    "# \tif cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "# \t\tbreak\n",
    "\n",
    "# # When all the process is done\n",
    "# # Release the capture and destroy all windows\n",
    "# capture.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6406134-e4b5-4ec2-baba-a9a2524baf06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "# # with different colors\n",
    "# import cv2\n",
    "# import mediapipe as mp\n",
    "# import time\n",
    "# import numpy as np\n",
    "\n",
    "# # Initializing mediapipe modules\n",
    "# mp_holistic = mp.solutions.holistic\n",
    "# mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# # Initialize holistic model\n",
    "# holistic_model = mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# # (0) in VideoCapture is used to connect to your computer's default camera\n",
    "# capture = cv2.VideoCapture(0)\n",
    "\n",
    "# # Initializing current time and previous time for calculating the FPS\n",
    "# previousTime = 0\n",
    "\n",
    "# # Define a list of 21 different colors for each hand landmark\n",
    "# colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0), (255, 0, 255), \n",
    "#           (0, 255, 255), (128, 0, 0), (0, 128, 0), (0, 0, 128), (128, 128, 0),\n",
    "#           (128, 0, 128), (0, 128, 128), (255, 128, 0), (255, 0, 128), (128, 255, 0),\n",
    "#           (0, 255, 128), (128, 0, 255), (0, 128, 255), (255, 128, 128), (128, 255, 128),\n",
    "#           (128, 128, 255)]\n",
    "\n",
    "# while capture.isOpened():\n",
    "#     # Capture frame by frame\n",
    "#     ret, frame = capture.read()\n",
    "\n",
    "#     # Resizing the frame for better view\n",
    "#     frame = cv2.resize(frame, (800, 600))\n",
    "\n",
    "#     # Converting the from BGR to RGB\n",
    "#     image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "#     # Making predictions using holistic model\n",
    "#     image.flags.writeable = False\n",
    "#     results = holistic_model.process(image)\n",
    "#     image.flags.writeable = True\n",
    "\n",
    "#     # Converting back the RGB image to BGR\n",
    "#     image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "#     # If landmarks are detected on the right hand\n",
    "#     if results.right_hand_landmarks:\n",
    "#         # Loop through each landmark point\n",
    "#         for idx, landmark in enumerate(results.right_hand_landmarks.landmark):\n",
    "#             # Get landmark coordinates\n",
    "#             h, w, c = image.shape\n",
    "#             cx, cy = int(landmark.x * w), int(landmark.y * h)\n",
    "\n",
    "#             # Draw circle for each landmark with a different color\n",
    "#             cv2.circle(image, (cx, cy), 7, colors[idx], cv2.FILLED)\n",
    "\n",
    "#         # Optionally, draw hand connections (in a single color if desired)\n",
    "#         mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "\n",
    "#     # Similarly, you can do this for the left hand\n",
    "#     if results.left_hand_landmarks:\n",
    "#         for idx, landmark in enumerate(results.left_hand_landmarks.landmark):\n",
    "#             h, w, c = image.shape\n",
    "#             cx, cy = int(landmark.x * w), int(landmark.y * h)\n",
    "#             cv2.circle(image, (cx, cy), 7, colors[idx], cv2.FILLED)\n",
    "\n",
    "#         mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "\n",
    "#     # Calculating the FPS\n",
    "#     currentTime = time.time()\n",
    "#     fps = 1 / (currentTime - previousTime)\n",
    "#     previousTime = currentTime\n",
    "\n",
    "#     # Displaying FPS on the image\n",
    "#     cv2.putText(image, str(int(fps)) + \" FPS\", (10, 70), cv2.FONT_HERSHEY_COMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "#     # Display the resulting image\n",
    "#     cv2.imshow(\"Hand Landmarks with Different Colors\", image)\n",
    "\n",
    "#     # Enter key 'q' to break the loop\n",
    "#     if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "#         break\n",
    "\n",
    "# # Release the capture and destroy all windows\n",
    "# capture.release()\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6854e3f-8395-4aaa-ac8c-8ca8a9e57bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1729373163.731524 2298054 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 83), renderer: Apple M2 Pro\n",
      "W0000 00:00:1729373163.774639 2301971 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1729373163.790078 2301971 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1729373163.792108 2301970 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1729373163.792235 2301968 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1729373163.792237 2301969 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1729373163.795530 2301974 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1729373163.801009 2301995 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1729373163.801014 2301968 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1729373163.803847 2301975 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1729373163.816773 2301997 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1729373163.818531 2301996 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1729373163.818652 2301995 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1729373163.818696 2301998 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1729373163.883355 2301995 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1729373163.888656 2301997 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1729373163.937355 2301996 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1729373166.024810 2301996 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n",
      "/Users/tiffany/anaconda3/lib/python3.11/site-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 22\u001b[0m\n\u001b[1;32m     18\u001b[0m currentTime \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m capture\u001b[38;5;241m.\u001b[39misOpened():\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# Capture frame by frame\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m     ret, frame \u001b[38;5;241m=\u001b[39m capture\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# Resizing the frame for better view\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     frame \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(frame, (\u001b[38;5;241m800\u001b[39m, \u001b[38;5;241m600\u001b[39m))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # hands with numbers\n",
    "# import cv2\n",
    "# import time\n",
    "# import mediapipe as mp\n",
    "\n",
    "# # Initialize MediaPipe components\n",
    "# mp_holistic = mp.solutions.holistic\n",
    "# mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# # Create a holistic model instance\n",
    "# holistic_model = mp_holistic.Holistic()\n",
    "\n",
    "# # (0) in VideoCapture is used to connect to your computer's default camera\n",
    "# capture = cv2.VideoCapture(0)\n",
    "\n",
    "# # Initializing current time and previous time for calculating the FPS\n",
    "# previousTime = 0\n",
    "# currentTime = 0\n",
    "\n",
    "# while capture.isOpened():\n",
    "#     # Capture frame by frame\n",
    "#     ret, frame = capture.read()\n",
    "\n",
    "#     # Resizing the frame for better view\n",
    "#     frame = cv2.resize(frame, (800, 600))\n",
    "\n",
    "#     # Converting the frame from BGR to RGB\n",
    "#     image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "#     # Making predictions using holistic model\n",
    "#     image.flags.writeable = False\n",
    "#     results = holistic_model.process(image)\n",
    "#     image.flags.writeable = True\n",
    "\n",
    "#     # Converting back the RGB image to BGR\n",
    "#     image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "#     # Drawing Right hand Landmarks\n",
    "#     if results.right_hand_landmarks:\n",
    "#         mp_drawing.draw_landmarks(\n",
    "#             image,\n",
    "#             results.right_hand_landmarks,\n",
    "#             mp_holistic.HAND_CONNECTIONS\n",
    "#         )\n",
    "#         # Adding landmark numbers to right hand\n",
    "#         for id, lm in enumerate(results.right_hand_landmarks.landmark):\n",
    "#             h, w, _ = image.shape\n",
    "#             cx, cy = int(lm.x * w), int(lm.y * h)\n",
    "#             cv2.circle(image, (cx, cy), 5, (0, 0, 255), -1)  # Red circle for landmarks\n",
    "#             cv2.putText(image, str(id), (cx, cy - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)  # Label each landmark\n",
    "\n",
    "#     # Drawing Left hand Landmarks\n",
    "#     if results.left_hand_landmarks:\n",
    "#         mp_drawing.draw_landmarks(\n",
    "#             image,\n",
    "#             results.left_hand_landmarks,\n",
    "#             mp_holistic.HAND_CONNECTIONS\n",
    "#         )\n",
    "#         # Adding landmark numbers to left hand\n",
    "#         for id, lm in enumerate(results.left_hand_landmarks.landmark):\n",
    "#             h, w, _ = image.shape\n",
    "#             cx, cy = int(lm.x * w), int(lm.y * h)\n",
    "#             cv2.circle(image, (cx, cy), 5, (0, 0, 255), -1)  # Red circle for landmarks\n",
    "#             cv2.putText(image, str(id), (cx, cy - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255, 255, 255), 1)  # Label each landmark\n",
    "\n",
    "#     # Calculating the FPS\n",
    "#     currentTime = time.time()\n",
    "#     fps = 1 / (currentTime - previousTime)\n",
    "#     previousTime = currentTime\n",
    "\n",
    "#     # Displaying FPS on the image\n",
    "#     cv2.putText(image, str(int(fps)) + \" FPS\", (10, 70), cv2.FONT_HERSHEY_COMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "#     # Display the resulting image\n",
    "#     cv2.imshow(\"Facial and Hand Landmarks\", image)\n",
    "\n",
    "#     # Enter key 'q' to break the loop\n",
    "#     if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "#         break\n",
    "\n",
    "# # When all the process is done\n",
    "# # Release the capture and destroy all windows\n",
    "# capture.release()\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0638990b-7690-4047-aad0-c1f1742852f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to access landmarks\n",
    "for landmark in mp_holistic.HandLandmark:\n",
    "    print(landmark, landmark.value)\n",
    " \n",
    "print(mp_holistic.HandLandmark.WRIST.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e44113e-7413-458f-964f-819251706baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## STEP 1: Import the necessary modules.\n",
    "# import mediapipe as mp\n",
    "# from mediapipe.tasks import python\n",
    "# from mediapipe.tasks.python import vision\n",
    "\n",
    "# STEP 2: Create an HandLandmarker object.\n",
    "# base_options = python.BaseOptions(model_asset_path='hand_landmarker.task')\n",
    "# options = vision.HandLandmarkerOptions(base_options=base_options,\n",
    "#                                        num_hands=2)\n",
    "# detector = vision.HandLandmarker.create_from_options(options)\n",
    "# STEP 3: Load the input image.\n",
    "# image = mp.Image.create_from_file(\"a1.jpg\")\n",
    "\n",
    "# STEP 4: Detect hand landmarks from the input image.\n",
    "# detection_result = detector.detect(image)\n",
    "\n",
    "# STEP 5: Process the classification result. In this case, visualize it.\n",
    "# annotated_image = draw_landmarks_on_image(image.numpy_view(), detection_result)\n",
    "# cv2_imshow(cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR))##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b389e446-1655-4218-a38b-3f7e5e44d267",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33905b57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ff31e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34559bfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
