{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93e2d637-7571-40d4-8dc6-d9177571c115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: pip\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python mediapipe msvc-runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04e2a2ee-27f2-4c55-8cea-b503fc3592ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c8f5226-a390-4742-ac77-da7e2f25a0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1729327032.612655 3475765 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M3\n"
     ]
    }
   ],
   "source": [
    "# Grabbing the Holistic Model from Mediapipe and\n",
    "# Initializing the Model\n",
    "mp_holistic = mp.solutions.holistic\n",
    "holistic_model = mp_holistic.Holistic(\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    " \n",
    "# Initializing the drawing utils for drawing the facial landmarks on image\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b224b585-6d68-4738-91da-27281a9ee900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # (0) in VideoCapture is used to connect to your computer's default camera\n",
    "# capture = cv2.VideoCapture(0)\n",
    "\n",
    "# # Initializing current time and precious time for calculating the FPS\n",
    "# previousTime = 0\n",
    "# currentTime = 0\n",
    "\n",
    "# while capture.isOpened():\n",
    "# \t# capture frame by frame\n",
    "# \tret, frame = capture.read()\n",
    "\n",
    "# \t# resizing the frame for better view\n",
    "# \tframe = cv2.resize(frame, (800, 600))\n",
    "\n",
    "# \t# Converting the from BGR to RGB\n",
    "# \timage = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# \t# Making predictions using holistic model\n",
    "# \t# To improve performance, optionally mark the image as not writeable to\n",
    "# \t# pass by reference.\n",
    "# \timage.flags.writeable = False\n",
    "# \tresults = holistic_model.process(image)\n",
    "# \timage.flags.writeable = True\n",
    "\n",
    "# \t# Converting back the RGB image to BGR\n",
    "# \timage = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# \t# Drawing the Facial Landmarks\n",
    "# \tmp_drawing.draw_landmarks(\n",
    "# \timage,\n",
    "# \tresults.face_landmarks,\n",
    "# \tmp_holistic.FACEMESH_CONTOURS,\n",
    "# \tmp_drawing.DrawingSpec(\n",
    "# \t\tcolor=(255,0,255),\n",
    "# \t\tthickness=1,\n",
    "# \t\tcircle_radius=1\n",
    "# \t),\n",
    "# \tmp_drawing.DrawingSpec(\n",
    "# \t\tcolor=(0,255,255),\n",
    "# \t\tthickness=1,\n",
    "# \t\tcircle_radius=1\n",
    "# \t)\n",
    "# \t)\n",
    "\n",
    "# \t# Drawing Right hand Land Marks\n",
    "# \tmp_drawing.draw_landmarks(\n",
    "# \timage, \n",
    "# \tresults.right_hand_landmarks, \n",
    "# \tmp_holistic.HAND_CONNECTIONS\n",
    "# \t)\n",
    "\n",
    "# \t# Drawing Left hand Land Marks\n",
    "# \tmp_drawing.draw_landmarks(\n",
    "# \timage, \n",
    "# \tresults.left_hand_landmarks, \n",
    "# \tmp_holistic.HAND_CONNECTIONS\n",
    "# \t)\n",
    "\t\n",
    "# \t# Calculating the FPS\n",
    "# \tcurrentTime = time.time()\n",
    "# \tfps = 1 / (currentTime-previousTime)\n",
    "# \tpreviousTime = currentTime\n",
    "\t\n",
    "# \t# Displaying FPS on the image\n",
    "# \tcv2.putText(image, str(int(fps))+\" FPS\", (10, 70), cv2.FONT_HERSHEY_COMPLEX, 1, (0,255,0), 2)\n",
    "\n",
    "# \t# Display the resulting image\n",
    "# \tcv2.imshow(\"Facial and Hand Landmarks\", image)\n",
    "\n",
    "# \t# Enter key 'q' to break the loop\n",
    "# \tif cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "# \t\tbreak\n",
    "\n",
    "# # When all the process is done\n",
    "# # Release the capture and destroy all windows\n",
    "# capture.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6406134-e4b5-4ec2-baba-a9a2524baf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # with different colors\n",
    "# import cv2\n",
    "# import mediapipe as mp\n",
    "# import time\n",
    "# import numpy as np\n",
    "\n",
    "# # Initializing mediapipe modules\n",
    "# mp_holistic = mp.solutions.holistic\n",
    "# mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# # Initialize holistic model\n",
    "# holistic_model = mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# # (0) in VideoCapture is used to connect to your computer's default camera\n",
    "# capture = cv2.VideoCapture(0)\n",
    "\n",
    "# # Initializing current time and previous time for calculating the FPS\n",
    "# previousTime = 0\n",
    "\n",
    "# # Define a list of 21 different colors for each hand landmark\n",
    "# colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0), (255, 0, 255), \n",
    "#           (0, 255, 255), (128, 0, 0), (0, 128, 0), (0, 0, 128), (128, 128, 0),\n",
    "#           (128, 0, 128), (0, 128, 128), (255, 128, 0), (255, 0, 128), (128, 255, 0),\n",
    "#           (0, 255, 128), (128, 0, 255), (0, 128, 255), (255, 128, 128), (128, 255, 128),\n",
    "#           (128, 128, 255)]\n",
    "\n",
    "# while capture.isOpened():\n",
    "#     # Capture frame by frame\n",
    "#     ret, frame = capture.read()\n",
    "\n",
    "#     # Resizing the frame for better view\n",
    "#     frame = cv2.resize(frame, (800, 600))\n",
    "\n",
    "#     # Converting the from BGR to RGB\n",
    "#     image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "#     # Making predictions using holistic model\n",
    "#     image.flags.writeable = False\n",
    "#     results = holistic_model.process(image)\n",
    "#     image.flags.writeable = True\n",
    "\n",
    "#     # Converting back the RGB image to BGR\n",
    "#     image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "#     # If landmarks are detected on the right hand\n",
    "#     if results.right_hand_landmarks:\n",
    "#         # Loop through each landmark point\n",
    "#         for idx, landmark in enumerate(results.right_hand_landmarks.landmark):\n",
    "#             # Get landmark coordinates\n",
    "#             h, w, c = image.shape\n",
    "#             cx, cy = int(landmark.x * w), int(landmark.y * h)\n",
    "\n",
    "#             # Draw circle for each landmark with a different color\n",
    "#             cv2.circle(image, (cx, cy), 7, colors[idx], cv2.FILLED)\n",
    "\n",
    "#         # Optionally, draw hand connections (in a single color if desired)\n",
    "#         mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "\n",
    "#     # Similarly, you can do this for the left hand\n",
    "#     if results.left_hand_landmarks:\n",
    "#         for idx, landmark in enumerate(results.left_hand_landmarks.landmark):\n",
    "#             h, w, c = image.shape\n",
    "#             cx, cy = int(landmark.x * w), int(landmark.y * h)\n",
    "#             cv2.circle(image, (cx, cy), 7, colors[idx], cv2.FILLED)\n",
    "\n",
    "#         mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "\n",
    "#     # Calculating the FPS\n",
    "#     currentTime = time.time()\n",
    "#     fps = 1 / (currentTime - previousTime)\n",
    "#     previousTime = currentTime\n",
    "\n",
    "#     # Displaying FPS on the image\n",
    "#     cv2.putText(image, str(int(fps)) + \" FPS\", (10, 70), cv2.FONT_HERSHEY_COMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "#     # Display the resulting image\n",
    "#     cv2.imshow(\"Hand Landmarks with Different Colors\", image)\n",
    "\n",
    "#     # Enter key 'q' to break the loop\n",
    "#     if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "#         break\n",
    "\n",
    "# # Release the capture and destroy all windows\n",
    "# capture.release()\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6854e3f-8395-4aaa-ac8c-8ca8a9e57bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1729327032.649943 3475765 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M3\n",
      "W0000 00:00:1729327032.716964 3476403 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1729327032.728918 3476409 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1729327032.732786 3476405 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1729327032.735289 3476405 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1729327032.735289 3476407 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1729327032.737195 3476401 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1729327032.748671 3476402 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1729327032.759293 3476415 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1729327032.762860 3476414 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1729327032.764031 3476410 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1729327032.764540 3476416 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1729327032.769728 3476405 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1729327032.770575 3476412 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1729327032.771267 3476408 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1729327032.790334 3476416 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1729327032.791181 3476414 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 22\u001b[0m\n\u001b[1;32m     18\u001b[0m currentTime \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m capture\u001b[38;5;241m.\u001b[39misOpened():\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# Capture frame by frame\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m     ret, frame \u001b[38;5;241m=\u001b[39m \u001b[43mcapture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# Resizing the frame for better view\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     frame \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(frame, (\u001b[38;5;241m800\u001b[39m, \u001b[38;5;241m600\u001b[39m))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# hands with numbers\n",
    "import cv2\n",
    "import time\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize MediaPipe components\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Create a holistic model instance\n",
    "holistic_model = mp_holistic.Holistic()\n",
    "\n",
    "# (0) in VideoCapture is used to connect to your computer's default camera\n",
    "capture = cv2.VideoCapture(0)\n",
    "\n",
    "# Initializing current time and previous time for calculating the FPS\n",
    "previousTime = 0\n",
    "currentTime = 0\n",
    "\n",
    "while capture.isOpened():\n",
    "    # Capture frame by frame\n",
    "    ret, frame = capture.read()\n",
    "\n",
    "    # Resizing the frame for better view\n",
    "    frame = cv2.resize(frame, (800, 600))\n",
    "\n",
    "    # Converting the frame from BGR to RGB\n",
    "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Making predictions using holistic model\n",
    "    image.flags.writeable = False\n",
    "    results = holistic_model.process(image)\n",
    "    image.flags.writeable = True\n",
    "\n",
    "    # Converting back the RGB image to BGR\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Drawing Right hand Landmarks\n",
    "    if results.right_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image,\n",
    "            results.right_hand_landmarks,\n",
    "            mp_holistic.HAND_CONNECTIONS\n",
    "        )\n",
    "        # Adding landmark numbers to right hand\n",
    "        for id, lm in enumerate(results.right_hand_landmarks.landmark):\n",
    "            h, w, _ = image.shape\n",
    "            cx, cy = int(lm.x * w), int(lm.y * h)\n",
    "            cv2.circle(image, (cx, cy), 5, (0, 0, 255), -1)  # Red circle for landmarks\n",
    "            cv2.putText(image, str(id), (cx, cy - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)  # Label each landmark\n",
    "\n",
    "    # Drawing Left hand Landmarks\n",
    "    if results.left_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image,\n",
    "            results.left_hand_landmarks,\n",
    "            mp_holistic.HAND_CONNECTIONS\n",
    "        )\n",
    "        # Adding landmark numbers to left hand\n",
    "        for id, lm in enumerate(results.left_hand_landmarks.landmark):\n",
    "            h, w, _ = image.shape\n",
    "            cx, cy = int(lm.x * w), int(lm.y * h)\n",
    "            cv2.circle(image, (cx, cy), 5, (0, 0, 255), -1)  # Red circle for landmarks\n",
    "            cv2.putText(image, str(id), (cx, cy - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255, 255, 255), 1)  # Label each landmark\n",
    "\n",
    "    # Calculating the FPS\n",
    "    currentTime = time.time()\n",
    "    fps = 1 / (currentTime - previousTime)\n",
    "    previousTime = currentTime\n",
    "\n",
    "    # Displaying FPS on the image\n",
    "    cv2.putText(image, str(int(fps)) + \" FPS\", (10, 70), cv2.FONT_HERSHEY_COMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Display the resulting image\n",
    "    cv2.imshow(\"Facial and Hand Landmarks\", image)\n",
    "\n",
    "    # Enter key 'q' to break the loop\n",
    "    if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When all the process is done\n",
    "# Release the capture and destroy all windows\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3363dc-e88d-414f-8c38-6db5d64a60c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (joint, x, y, z)\n",
    "# import cv2\n",
    "# import time\n",
    "# import mediapipe as mp\n",
    "\n",
    "# # Initialize MediaPipe components\n",
    "# mp_holistic = mp.solutions.holistic\n",
    "# mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# # Create a holistic model instance\n",
    "# holistic_model = mp_holistic.Holistic()\n",
    "\n",
    "# # (0) in VideoCapture is used to connect to your computer's default camera\n",
    "# capture = cv2.VideoCapture(0)\n",
    "\n",
    "# while capture.isOpened():\n",
    "#     # Capture frame by frame\n",
    "#     ret, frame = capture.read()\n",
    "\n",
    "#     # Converting the frame from BGR to RGB\n",
    "#     image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "#     # Making predictions using holistic model\n",
    "#     image.flags.writeable = False\n",
    "#     results = holistic_model.process(image)\n",
    "#     image.flags.writeable = True\n",
    "\n",
    "#     # Extracting right hand landmarks if available\n",
    "#     if results.right_hand_landmarks:\n",
    "#         right_hand_joints = []\n",
    "#         for id, lm in enumerate(results.right_hand_landmarks.landmark):\n",
    "#             right_hand_joints.append((id, lm.x, lm.y, lm.z))  # Store the landmark ID and normalized coordinates\n",
    "#         print(\"Right Hand Joints:\", right_hand_joints)\n",
    "\n",
    "#     # Extracting left hand landmarks if available\n",
    "#     if results.left_hand_landmarks:\n",
    "#         left_hand_joints = []\n",
    "#         for id, lm in enumerate(results.left_hand_landmarks.landmark):\n",
    "#             left_hand_joints.append((id, lm.x, lm.y, lm.z))  # Store the landmark ID and normalized coordinates\n",
    "#         print(\"Left Hand Joints:\", left_hand_joints)\n",
    "\n",
    "#     # Enter key 'q' to break the loop\n",
    "#     if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "#         break\n",
    "\n",
    "# # When all the process is done\n",
    "# # Release the capture\n",
    "# capture.release()\n",
    "# cv2.destroyAllWindows()\n",
    "# # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0638990b-7690-4047-aad0-c1f1742852f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to access landmarks\n",
    "for landmark in mp_holistic.HandLandmark:\n",
    "    print(landmark, landmark.value)\n",
    " \n",
    "print(mp_holistic.HandLandmark.WRIST.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e44113e-7413-458f-964f-819251706baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## STEP 1: Import the necessary modules.\n",
    "# import mediapipe as mp\n",
    "# from mediapipe.tasks import python\n",
    "# from mediapipe.tasks.python import vision\n",
    "\n",
    "# STEP 2: Create an HandLandmarker object.\n",
    "# base_options = python.BaseOptions(model_asset_path='hand_landmarker.task')\n",
    "# options = vision.HandLandmarkerOptions(base_options=base_options,\n",
    "#                                        num_hands=2)\n",
    "# detector = vision.HandLandmarker.create_from_options(options)\n",
    "# STEP 3: Load the input image.\n",
    "# image = mp.Image.create_from_file(\"a1.jpg\")\n",
    "\n",
    "# STEP 4: Detect hand landmarks from the input image.\n",
    "# detection_result = detector.detect(image)\n",
    "\n",
    "# STEP 5: Process the classification result. In this case, visualize it.\n",
    "# annotated_image = draw_landmarks_on_image(image.numpy_view(), detection_result)\n",
    "# cv2_imshow(cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR))##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b389e446-1655-4218-a38b-3f7e5e44d267",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
