{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93e2d637-7571-40d4-8dc6-d9177571c115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\python312\\lib\\site-packages (4.10.0.84)\n",
      "Collecting mediapipe\n",
      "  Using cached mediapipe-0.10.14-cp312-cp312-win_amd64.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: msvc-runtime in c:\\python312\\lib\\site-packages (14.40.33807)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\python312\\lib\\site-packages (from opencv-python) (2.0.0)\n",
      "Collecting absl-py (from mediapipe)\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting attrs>=19.1.0 (from mediapipe)\n",
      "  Using cached attrs-24.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\python312\\lib\\site-packages (from mediapipe) (24.3.25)\n",
      "Collecting jax (from mediapipe)\n",
      "  Using cached jax-0.4.34-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib (from mediapipe)\n",
      "  Using cached jaxlib-0.4.34-cp312-cp312-win_amd64.whl.metadata (1.0 kB)\n",
      "Collecting matplotlib (from mediapipe)\n",
      "  Using cached matplotlib-3.9.2-cp312-cp312-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\python312\\lib\\site-packages (from mediapipe) (4.10.0.84)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in c:\\python312\\lib\\site-packages (from mediapipe) (4.25.3)\n",
      "Collecting sounddevice>=0.4.4 (from mediapipe)\n",
      "  Using cached sounddevice-0.5.1-py3-none-win_amd64.whl.metadata (1.4 kB)\n",
      "Collecting CFFI>=1.0 (from sounddevice>=0.4.4->mediapipe)\n",
      "  Using cached cffi-1.17.1-cp312-cp312-win_amd64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in c:\\python312\\lib\\site-packages (from jax->mediapipe) (0.5.0)\n",
      "Requirement already satisfied: opt-einsum in c:\\python312\\lib\\site-packages (from jax->mediapipe) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.10 in c:\\python312\\lib\\site-packages (from jax->mediapipe) (1.14.1)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib->mediapipe)\n",
      "  Using cached contourpy-1.3.0-cp312-cp312-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib->mediapipe)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\python312\\lib\\site-packages (from matplotlib->mediapipe) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\python312\\lib\\site-packages (from matplotlib->mediapipe) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\python312\\lib\\site-packages (from matplotlib->mediapipe) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\python312\\lib\\site-packages (from matplotlib->mediapipe) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\python312\\lib\\site-packages (from matplotlib->mediapipe) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\kouro\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
      "Requirement already satisfied: pycparser in c:\\python312\\lib\\site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
      "Requirement already satisfied: six>=1.5 in c:\\python312\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
      "Using cached mediapipe-0.10.14-cp312-cp312-win_amd64.whl (50.8 MB)\n",
      "Using cached attrs-24.2.0-py3-none-any.whl (63 kB)\n",
      "Using cached sounddevice-0.5.1-py3-none-win_amd64.whl (363 kB)\n",
      "Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Using cached jax-0.4.34-py3-none-any.whl (2.1 MB)\n",
      "Using cached jaxlib-0.4.34-cp312-cp312-win_amd64.whl (55.3 MB)\n",
      "Using cached matplotlib-3.9.2-cp312-cp312-win_amd64.whl (7.8 MB)\n",
      "Using cached cffi-1.17.1-cp312-cp312-win_amd64.whl (181 kB)\n",
      "Using cached contourpy-1.3.0-cp312-cp312-win_amd64.whl (218 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Installing collected packages: cycler, contourpy, CFFI, attrs, absl-py, sounddevice, matplotlib, jaxlib, jax, mediapipe\n",
      "Successfully installed CFFI-1.17.1 absl-py-2.1.0 attrs-24.2.0 contourpy-1.3.0 cycler-0.12.1 jax-0.4.34 jaxlib-0.4.34 matplotlib-3.9.2 mediapipe-0.10.14 sounddevice-0.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (c:\\Python312\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "%pip install --user opencv-python mediapipe msvc-runtime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04e2a2ee-27f2-4c55-8cea-b503fc3592ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c8f5226-a390-4742-ac77-da7e2f25a0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing the Holistic Model from Mediapipe and\n",
    "# Initializing the Model\n",
    "mp_holistic = mp.solutions.holistic\n",
    "holistic_model = mp_holistic.Holistic(\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    " \n",
    "# Initializing the drawing utils for drawing the facial landmarks on image\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b224b585-6d68-4738-91da-27281a9ee900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # (0) in VideoCapture is used to connect to your computer's default camera\n",
    "# capture = cv2.VideoCapture(0)\n",
    "\n",
    "# # Initializing current time and precious time for calculating the FPS\n",
    "# previousTime = 0\n",
    "# currentTime = 0\n",
    "\n",
    "# while capture.isOpened():\n",
    "# \t# capture frame by frame\n",
    "# \tret, frame = capture.read()\n",
    "\n",
    "# \t# resizing the frame for better view\n",
    "# \tframe = cv2.resize(frame, (800, 600))\n",
    "\n",
    "# \t# Converting the from BGR to RGB\n",
    "# \timage = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# \t# Making predictions using holistic model\n",
    "# \t# To improve performance, optionally mark the image as not writeable to\n",
    "# \t# pass by reference.\n",
    "# \timage.flags.writeable = False\n",
    "# \tresults = holistic_model.process(image)\n",
    "# \timage.flags.writeable = True\n",
    "\n",
    "# \t# Converting back the RGB image to BGR\n",
    "# \timage = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# \t# Drawing the Facial Landmarks\n",
    "# \tmp_drawing.draw_landmarks(\n",
    "# \timage,\n",
    "# \tresults.face_landmarks,\n",
    "# \tmp_holistic.FACEMESH_CONTOURS,\n",
    "# \tmp_drawing.DrawingSpec(\n",
    "# \t\tcolor=(255,0,255),\n",
    "# \t\tthickness=1,\n",
    "# \t\tcircle_radius=1\n",
    "# \t),\n",
    "# \tmp_drawing.DrawingSpec(\n",
    "# \t\tcolor=(0,255,255),\n",
    "# \t\tthickness=1,\n",
    "# \t\tcircle_radius=1\n",
    "# \t)\n",
    "# \t)\n",
    "\n",
    "# \t# Drawing Right hand Land Marks\n",
    "# \tmp_drawing.draw_landmarks(\n",
    "# \timage, \n",
    "# \tresults.right_hand_landmarks, \n",
    "# \tmp_holistic.HAND_CONNECTIONS\n",
    "# \t)\n",
    "\n",
    "# \t# Drawing Left hand Land Marks\n",
    "# \tmp_drawing.draw_landmarks(\n",
    "# \timage, \n",
    "# \tresults.left_hand_landmarks, \n",
    "# \tmp_holistic.HAND_CONNECTIONS\n",
    "# \t)\n",
    "\t\n",
    "# \t# Calculating the FPS\n",
    "# \tcurrentTime = time.time()\n",
    "# \tfps = 1 / (currentTime-previousTime)\n",
    "# \tpreviousTime = currentTime\n",
    "\t\n",
    "# \t# Displaying FPS on the image\n",
    "# \tcv2.putText(image, str(int(fps))+\" FPS\", (10, 70), cv2.FONT_HERSHEY_COMPLEX, 1, (0,255,0), 2)\n",
    "\n",
    "# \t# Display the resulting image\n",
    "# \tcv2.imshow(\"Facial and Hand Landmarks\", image)\n",
    "\n",
    "# \t# Enter key 'q' to break the loop\n",
    "# \tif cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "# \t\tbreak\n",
    "\n",
    "# # When all the process is done\n",
    "# # Release the capture and destroy all windows\n",
    "# capture.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6406134-e4b5-4ec2-baba-a9a2524baf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # with different colors\n",
    "# import cv2\n",
    "# import mediapipe as mp\n",
    "# import time\n",
    "# import numpy as np\n",
    "\n",
    "# # Initializing mediapipe modules\n",
    "# mp_holistic = mp.solutions.holistic\n",
    "# mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# # Initialize holistic model\n",
    "# holistic_model = mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# # (0) in VideoCapture is used to connect to your computer's default camera\n",
    "# capture = cv2.VideoCapture(0)\n",
    "\n",
    "# # Initializing current time and previous time for calculating the FPS\n",
    "# previousTime = 0\n",
    "\n",
    "# # Define a list of 21 different colors for each hand landmark\n",
    "# colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0), (255, 0, 255), \n",
    "#           (0, 255, 255), (128, 0, 0), (0, 128, 0), (0, 0, 128), (128, 128, 0),\n",
    "#           (128, 0, 128), (0, 128, 128), (255, 128, 0), (255, 0, 128), (128, 255, 0),\n",
    "#           (0, 255, 128), (128, 0, 255), (0, 128, 255), (255, 128, 128), (128, 255, 128),\n",
    "#           (128, 128, 255)]\n",
    "\n",
    "# while capture.isOpened():\n",
    "#     # Capture frame by frame\n",
    "#     ret, frame = capture.read()\n",
    "\n",
    "#     # Resizing the frame for better view\n",
    "#     frame = cv2.resize(frame, (800, 600))\n",
    "\n",
    "#     # Converting the from BGR to RGB\n",
    "#     image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "#     # Making predictions using holistic model\n",
    "#     image.flags.writeable = False\n",
    "#     results = holistic_model.process(image)\n",
    "#     image.flags.writeable = True\n",
    "\n",
    "#     # Converting back the RGB image to BGR\n",
    "#     image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "#     # If landmarks are detected on the right hand\n",
    "#     if results.right_hand_landmarks:\n",
    "#         # Loop through each landmark point\n",
    "#         for idx, landmark in enumerate(results.right_hand_landmarks.landmark):\n",
    "#             # Get landmark coordinates\n",
    "#             h, w, c = image.shape\n",
    "#             cx, cy = int(landmark.x * w), int(landmark.y * h)\n",
    "\n",
    "#             # Draw circle for each landmark with a different color\n",
    "#             cv2.circle(image, (cx, cy), 7, colors[idx], cv2.FILLED)\n",
    "\n",
    "#         # Optionally, draw hand connections (in a single color if desired)\n",
    "#         mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "\n",
    "#     # Similarly, you can do this for the left hand\n",
    "#     if results.left_hand_landmarks:\n",
    "#         for idx, landmark in enumerate(results.left_hand_landmarks.landmark):\n",
    "#             h, w, c = image.shape\n",
    "#             cx, cy = int(landmark.x * w), int(landmark.y * h)\n",
    "#             cv2.circle(image, (cx, cy), 7, colors[idx], cv2.FILLED)\n",
    "\n",
    "#         mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "\n",
    "#     # Calculating the FPS\n",
    "#     currentTime = time.time()\n",
    "#     fps = 1 / (currentTime - previousTime)\n",
    "#     previousTime = currentTime\n",
    "\n",
    "#     # Displaying FPS on the image\n",
    "#     cv2.putText(image, str(int(fps)) + \" FPS\", (10, 70), cv2.FONT_HERSHEY_COMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "#     # Display the resulting image\n",
    "#     cv2.imshow(\"Hand Landmarks with Different Colors\", image)\n",
    "\n",
    "#     # Enter key 'q' to break the loop\n",
    "#     if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "#         break\n",
    "\n",
    "# # Release the capture and destroy all windows\n",
    "# capture.release()\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6854e3f-8395-4aaa-ac8c-8ca8a9e57bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hands with numbers\n",
    "import cv2\n",
    "import time\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize MediaPipe components\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Create a holistic model instance\n",
    "holistic_model = mp_holistic.Holistic()\n",
    "\n",
    "# (0) in VideoCapture is used to connect to your computer's default camera\n",
    "capture = cv2.VideoCapture(0)\n",
    "\n",
    "# Initializing current time and previous time for calculating the FPS\n",
    "previousTime = 0\n",
    "currentTime = 0\n",
    "\n",
    "while capture.isOpened():\n",
    "    # Capture frame by frame\n",
    "    ret, frame = capture.read()\n",
    "\n",
    "    # Resizing the frame for better view\n",
    "    frame = cv2.resize(frame, (800, 600))\n",
    "\n",
    "    # Converting the frame from BGR to RGB\n",
    "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Making predictions using holistic model\n",
    "    image.flags.writeable = False\n",
    "    results = holistic_model.process(image)\n",
    "    image.flags.writeable = True\n",
    "\n",
    "    # Converting back the RGB image to BGR\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Drawing Right hand Landmarks\n",
    "    if results.right_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image,\n",
    "            results.right_hand_landmarks,\n",
    "            mp_holistic.HAND_CONNECTIONS\n",
    "        )\n",
    "        # Adding landmark numbers to right hand\n",
    "        for id, lm in enumerate(results.right_hand_landmarks.landmark):\n",
    "            h, w, _ = image.shape\n",
    "            cx, cy = int(lm.x * w), int(lm.y * h)\n",
    "            cv2.circle(image, (cx, cy), 5, (0, 0, 255), -1)  # Red circle for landmarks\n",
    "            cv2.putText(image, str(id), (cx, cy - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)  # Label each landmark\n",
    "\n",
    "    # Drawing Left hand Landmarks\n",
    "    if results.left_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image,\n",
    "            results.left_hand_landmarks,\n",
    "            mp_holistic.HAND_CONNECTIONS\n",
    "        )\n",
    "        # Adding landmark numbers to left hand\n",
    "        for id, lm in enumerate(results.left_hand_landmarks.landmark):\n",
    "            h, w, _ = image.shape\n",
    "            cx, cy = int(lm.x * w), int(lm.y * h)\n",
    "            cv2.circle(image, (cx, cy), 5, (0, 0, 255), -1)  # Red circle for landmarks\n",
    "            cv2.putText(image, str(id), (cx, cy - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255, 255, 255), 1)  # Label each landmark\n",
    "\n",
    "    # Calculating the FPS\n",
    "    currentTime = time.time()\n",
    "    fps = 1 / (currentTime - previousTime)\n",
    "    previousTime = currentTime\n",
    "\n",
    "    # Displaying FPS on the image\n",
    "    cv2.putText(image, str(int(fps)) + \" FPS\", (10, 70), cv2.FONT_HERSHEY_COMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Display the resulting image\n",
    "    cv2.imshow(\"Facial and Hand Landmarks\", image)\n",
    "\n",
    "    # Enter key 'q' to break the loop\n",
    "    if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When all the process is done\n",
    "# Release the capture and destroy all windows\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3363dc-e88d-414f-8c38-6db5d64a60c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (joint, x, y, z)\n",
    "# import cv2\n",
    "# import time\n",
    "# import mediapipe as mp\n",
    "\n",
    "# # Initialize MediaPipe components\n",
    "# mp_holistic = mp.solutions.holistic\n",
    "# mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# # Create a holistic model instance\n",
    "# holistic_model = mp_holistic.Holistic()\n",
    "\n",
    "# # (0) in VideoCapture is used to connect to your computer's default camera\n",
    "# capture = cv2.VideoCapture(0)\n",
    "\n",
    "# while capture.isOpened():\n",
    "#     # Capture frame by frame\n",
    "#     ret, frame = capture.read()\n",
    "\n",
    "#     # Converting the frame from BGR to RGB\n",
    "#     image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "#     # Making predictions using holistic model\n",
    "#     image.flags.writeable = False\n",
    "#     results = holistic_model.process(image)\n",
    "#     image.flags.writeable = True\n",
    "\n",
    "#     # Extracting right hand landmarks if available\n",
    "#     if results.right_hand_landmarks:\n",
    "#         right_hand_joints = []\n",
    "#         for id, lm in enumerate(results.right_hand_landmarks.landmark):\n",
    "#             right_hand_joints.append((id, lm.x, lm.y, lm.z))  # Store the landmark ID and normalized coordinates\n",
    "#         print(\"Right Hand Joints:\", right_hand_joints)\n",
    "\n",
    "#     # Extracting left hand landmarks if available\n",
    "#     if results.left_hand_landmarks:\n",
    "#         left_hand_joints = []\n",
    "#         for id, lm in enumerate(results.left_hand_landmarks.landmark):\n",
    "#             left_hand_joints.append((id, lm.x, lm.y, lm.z))  # Store the landmark ID and normalized coordinates\n",
    "#         print(\"Left Hand Joints:\", left_hand_joints)\n",
    "\n",
    "#     # Enter key 'q' to break the loop\n",
    "#     if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "#         break\n",
    "\n",
    "# # When all the process is done\n",
    "# # Release the capture\n",
    "# capture.release()\n",
    "# cv2.destroyAllWindows()\n",
    "# # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0638990b-7690-4047-aad0-c1f1742852f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to access landmarks\n",
    "for landmark in mp_holistic.HandLandmark:\n",
    "    print(landmark, landmark.value)\n",
    " \n",
    "print(mp_holistic.HandLandmark.WRIST.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e44113e-7413-458f-964f-819251706baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## STEP 1: Import the necessary modules.\n",
    "# import mediapipe as mp\n",
    "# from mediapipe.tasks import python\n",
    "# from mediapipe.tasks.python import vision\n",
    "\n",
    "# STEP 2: Create an HandLandmarker object.\n",
    "# base_options = python.BaseOptions(model_asset_path='hand_landmarker.task')\n",
    "# options = vision.HandLandmarkerOptions(base_options=base_options,\n",
    "#                                        num_hands=2)\n",
    "# detector = vision.HandLandmarker.create_from_options(options)\n",
    "# STEP 3: Load the input image.\n",
    "# image = mp.Image.create_from_file(\"a1.jpg\")\n",
    "\n",
    "# STEP 4: Detect hand landmarks from the input image.\n",
    "# detection_result = detector.detect(image)\n",
    "\n",
    "# STEP 5: Process the classification result. In this case, visualize it.\n",
    "# annotated_image = draw_landmarks_on_image(image.numpy_view(), detection_result)\n",
    "# cv2_imshow(cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR))##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b389e446-1655-4218-a38b-3f7e5e44d267",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
