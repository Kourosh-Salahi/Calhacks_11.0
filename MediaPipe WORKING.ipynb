{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93e2d637-7571-40d4-8dc6-d9177571c115",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-8-840f382e1e4b>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-840f382e1e4b>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    pip install opencv-python mediapipe msvc-runtime\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python mediapipe msvc-runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e5cfd84",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-e33e4ab0c104>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-e33e4ab0c104>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    pip install mediapipe\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04e2a2ee-27f2-4c55-8cea-b503fc3592ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "required field \"type_ignores\" missing from Module",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/codeop.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, source, filename, symbol)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0mcodeob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcodeob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mco_flags\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompiler_flag\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: required field \"type_ignores\" missing from Module"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c8f5226-a390-4742-ac77-da7e2f25a0b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "required field \"type_ignores\" missing from Module",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/codeop.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, source, filename, symbol)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0mcodeob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcodeob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mco_flags\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompiler_flag\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: required field \"type_ignores\" missing from Module"
     ]
    }
   ],
   "source": [
    "# Grabbing the Holistic Model from Mediapipe and\n",
    "# Initializing the Model\n",
    "mp_holistic = mp.solutions.holistic\n",
    "holistic_model = mp_holistic.Holistic(\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    " \n",
    "# Initializing the drawing utils for drawing the facial landmarks on image\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b086fbe6-f8c9-4482-a289-ffcc8c0104e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Gemini Test\n",
    "# %pip install gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18c45b52-42ba-44a7-8460-80fb82aaa87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gemini\n",
    "# response = gemini.generate_text(prompt=\"Tell me a joke\")\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd256618-afdf-453d-adea-d927f64e1575",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "required field \"type_ignores\" missing from Module",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/codeop.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, source, filename, symbol)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0mcodeob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcodeob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mco_flags\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompiler_flag\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: required field \"type_ignores\" missing from Module"
     ]
    }
   ],
   "source": [
    "#WORKING for Groq\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import re\n",
    "from groq import Groq\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Initialize MediaPipe components\n",
    "mp_holistic = mp.solutions.holistic\n",
    "\n",
    "# Create a holistic model instance\n",
    "holistic_model = mp_holistic.Holistic()\n",
    "\n",
    "# Video capture from the default camera\n",
    "capture = cv2.VideoCapture(0)\n",
    "\n",
    "# Define frame-skip rate (e.g., process 1 out of every 'n' frames)\n",
    "frame_skip = 2\n",
    "frame_count = 0\n",
    "\n",
    "# Initialize API client\n",
    "key = \"\"\n",
    "client = Groq(api_key=key)\n",
    "\n",
    "# Variable to store ASL letters\n",
    "result = \"\"\n",
    "\n",
    "while capture.isOpened():\n",
    "    ret, frame = capture.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Increment the frame count\n",
    "    frame_count += 1\n",
    "\n",
    "    # Skip frames based on the defined frame_skip rate\n",
    "    if frame_count % frame_skip != 0:\n",
    "        continue\n",
    "\n",
    "    # Converting the frame from BGR to RGB\n",
    "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Making predictions using holistic model\n",
    "    image.flags.writeable = False\n",
    "    results = holistic_model.process(image)\n",
    "    image.flags.writeable = True\n",
    "\n",
    "    hand_joints = []\n",
    "\n",
    "    # Extracting right hand landmarks if available\n",
    "    if results.right_hand_landmarks:\n",
    "        right_hand_joints = [\n",
    "            (id, lm.x, lm.y, lm.z)\n",
    "            for id, lm in enumerate(results.right_hand_landmarks.landmark)\n",
    "        ]\n",
    "        hand_joints.append(right_hand_joints)\n",
    "\n",
    "    # Extracting left hand landmarks if available\n",
    "    if results.left_hand_landmarks:\n",
    "        left_hand_joints = [\n",
    "            (id, lm.x, lm.y, lm.z)\n",
    "            for id, lm in enumerate(results.left_hand_landmarks.landmark)\n",
    "        ]\n",
    "        hand_joints.append(left_hand_joints)\n",
    "\n",
    "    # Prepare content for the API request\n",
    "    if hand_joints:\n",
    "        content = (\n",
    "            f\"{hand_joints} Each of these points is a number followed by a 3D vector \"\n",
    "            f\"(number, x, y, z), each of the numbers corresponds to a joint on the hand. \"\n",
    "            f\"0. WRIST 1. THUMB_CMC 2. THUMB_MCP 3. THUMB_IP 4. THUMB_TIP \"\n",
    "            f\"5. INDEX_FINGER_MCP 6. INDEX_FINGER_PIP 7. INDEX_FINGER_DIP 8. INDEX_FINGER_TIP \"\n",
    "            f\"9. MIDDLE_FINGER_MCP 10. MIDDLE_FINGER_PIP 11. MIDDLE_FINGER_DIP 12. MIDDLE_FINGER_TIP \"\n",
    "            f\"13. RING_FINGER_MCP 14. RING_FINGER_PIP 15. RING_FINGER_DIP 16. RING_FINGER_TIP \"\n",
    "            f\"17. PINKY_MCP 18. PINKY_PIP 19. PINKY_DIP 20. PINKY_TIP. \"\n",
    "            f\"What ASL letter does this create? Please return the answer in the format 'ASL Letter: 'X''.\"\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            completion = client.chat.completions.create(\n",
    "                model=\"llama-3.2-90b-vision-preview\",\n",
    "                messages=[{\"role\": \"user\", \"content\": content}],\n",
    "                temperature=0.5,\n",
    "                max_tokens=500,\n",
    "                top_p=1,\n",
    "                stream=False,\n",
    "            )\n",
    "\n",
    "            text = completion.choices[0].message.content\n",
    "            #print(text)\n",
    "\n",
    "            # Extract ASL letter from the API response\n",
    "            # ASL_letter_match = re.search(r'ASL Letter: [\"\\']([A-Z])[\"\\']', text)\n",
    "            # if ASL_letter_match:\n",
    "            #     asl_letter = ASL_letter_match.group(1)\n",
    "            #     result += asl_letter\n",
    "            #     print(result)  # Print the updated result\n",
    "\n",
    "            ASL_letter_match = re.search(r'ASL Letter:\\s*[\"\\']?([A-Z])[\"\\']?', text)\n",
    "\n",
    "            if ASL_letter_match:\n",
    "                asl_letter = ASL_letter_match.group(1)\n",
    "                result += asl_letter\n",
    "                print(result)\n",
    "            else:\n",
    "                print(\"No ASL letter found in the response.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error with API response: {e}\")\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Enter key 'q' to break the loop\n",
    "    if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the capture and destroy windows\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34f0dc4-ecd5-4458-9c19-8504f38bed74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # (joint, x, y, z) with frame-skip\n",
    "\n",
    "# import cv2\n",
    "# import mediapipe as mp\n",
    "# from groq import Groq\n",
    "# import re\n",
    "\n",
    "# # Initialize MediaPipe components\n",
    "# mp_holistic = mp.solutions.holistic\n",
    "# mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# # Create a holistic model instance\n",
    "# holistic_model = mp_holistic.Holistic()\n",
    "\n",
    "# # (0) in VideoCapture is used to connect to your computer's default camera\n",
    "# capture = cv2.VideoCapture(0)\n",
    "\n",
    "# # Define frame-skip rate (e.g., process 1 out of every 'n' frames)\n",
    "# frame_skip = 5\n",
    "# frame_count = 0\n",
    "\n",
    "# result = \"\"\n",
    "\n",
    "# while capture.isOpened():\n",
    "#     # Capture frame by frame\n",
    "#     ret, frame = capture.read()\n",
    "\n",
    "#     if not ret:\n",
    "#         break\n",
    "\n",
    "#     # Increment the frame count\n",
    "#     frame_count += 1\n",
    "\n",
    "#     # Skip frames based on the defined frame_skip rate\n",
    "#     if frame_count % frame_skip != 0:\n",
    "#         continue\n",
    "\n",
    "#     # Converting the frame from BGR to RGB\n",
    "#     image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "#     # Making predictions using holistic model\n",
    "#     image.flags.writeable = False\n",
    "#     results = holistic_model.process(image)\n",
    "#     image.flags.writeable = True\n",
    "\n",
    "#     hand_joints = []\n",
    "\n",
    "#     # Extracting right hand landmarks if available\n",
    "#     if results.right_hand_landmarks:\n",
    "#         right_hand_joints = []\n",
    "#         for id, lm in enumerate(results.right_hand_landmarks.landmark):\n",
    "#             right_hand_joints.append((id, lm.x, lm.y, lm.z))  # Store the landmark ID and normalized coordinates\n",
    "#         hand_joints.append(right_hand_joints)\n",
    "#         #print(\"Right Hand Joints:\", right_hand_joints)\n",
    "\n",
    "#     # Extracting left hand landmarks if available\n",
    "#     if results.left_hand_landmarks:\n",
    "#         left_hand_joints = []\n",
    "#         for id, lm in enumerate(results.left_hand_landmarks.landmark):\n",
    "#             left_hand_joints.append((id, lm.x, lm.y, lm.z))  # Store the landmark ID and normalized coordinates\n",
    "#         hand_joints.append(left_hand_joints)\n",
    "#         #print(\"Left Hand Joints:\", left_hand_joints)\n",
    "\n",
    "#     content = str(hand_joints) + \"Each of these points is a number followed by a 3 dimensional vector in space (number, x, y, z), each of the numbers corresponds to a joint on the hand. 0. WRIST 1. THUMB_CMC 2. THUMB_MCP  3. THUMB_IP 4. THUMB_TIP 5. INDEX FINGER_MCP 6. INDEX FINGER PIP 7. INDEX FINGER_DIP 8. INDEX FINGER TIP 9. MIDDLE FINGER_MCP 10. MIDDLE FINGER_PIP 11. MIDDLE FINGER DIP 12. MIDDLE FINGER TIP 13. RING FINGER MCP 14. RING FINGER PIP 15. RING FINGER_DIP 16. RING FINGER TIP 17. PINKY MCP 18. PINKY PIP 19. PINKY DIP 20. PINKY TIP What ASL letter does this create. Please return the answer in the format 'ASL Letter: 'X''.\"\n",
    "    \n",
    "#     if len(hand_joints) > 0: \n",
    "#         GROQ_API_KEY = \"\"\n",
    "#         client = Groq(api_key=GROQ_API_KEY)\n",
    "#         completion = client.chat.completions.create(\n",
    "#             model=\"llama-3.2-90b-vision-preview\",\n",
    "#             messages=[\n",
    "#                 {\n",
    "#                     \"role\": \"user\",\n",
    "#                     \"content\": content\n",
    "#                 }\n",
    "#             ],\n",
    "#             temperature=1,\n",
    "#             max_tokens=1024,\n",
    "#             top_p=1,\n",
    "#             stream=False,\n",
    "#             stop=None,\n",
    "#         )\n",
    "#         #print(completion.choices[0].message)\n",
    "        \n",
    "#         text = completion.choices[0].message.content\n",
    "#         #print(text + \"\\n\")\n",
    "        \n",
    "#         ASL_letter = re.search(r'ASL Letter: [\"\\']([A-Z])[\"\\']', text)\n",
    "#         if ASL_letter:\n",
    "#             asl_letter = ASL_letter.group(1)\n",
    "#             print(asl_letter)\n",
    "    \n",
    "#         # if len(capital_letters) == 1: \n",
    "#         #     final_letters += capital_letters\n",
    "#         # elif len(capital_letters) > 1:\n",
    "#         #     unique_letters = []\n",
    "#         #     for letter in capital_letters:\n",
    "#         #         if letter.isalpha() and letter not in unique_letters:\n",
    "#         #                 unique_letters.append(letter)\n",
    "#         #     final_letters += unique_letters\n",
    "\n",
    "#         # if ASL_letter:\n",
    "#         #     result = result + ASL_letter\n",
    "#         #     print(result)\n",
    "        \n",
    "#         hand_joints.clear()\n",
    "\n",
    "#     # Enter key 'q' to break the loop\n",
    "#     if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "#         break\n",
    "\n",
    "# # When all the process is done\n",
    "# # Release the capture\n",
    "# capture.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3363dc-e88d-414f-8c38-6db5d64a60c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # (joint, x, y, z)\n",
    "# import cv2\n",
    "# import time\n",
    "# import mediapipe as mp\n",
    "# from groq import Groq\n",
    "\n",
    "# # Initialize MediaPipe components\n",
    "# mp_holistic = mp.solutions.holistic\n",
    "# mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# # Create a holistic model instance\n",
    "# holistic_model = mp_holistic.Holistic()\n",
    "\n",
    "# # (0) in VideoCapture is used to connect to your computer's default camera\n",
    "# capture = cv2.VideoCapture(0)\n",
    "\n",
    "# while capture.isOpened():\n",
    "#     # Capture frame by frame\n",
    "#     ret, frame = capture.read()\n",
    "\n",
    "#     # Converting the frame from BGR to RGB\n",
    "#     image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "#     # Making predictions using holistic model\n",
    "#     image.flags.writeable = False\n",
    "#     results = holistic_model.process(image)\n",
    "#     image.flags.writeable = True\n",
    "\n",
    "#     hand_joints = []\n",
    "\n",
    "#     # Extracting right hand landmarks if available\n",
    "#     if results.right_hand_landmarks:\n",
    "#         right_hand_joints = []\n",
    "#         for id, lm in enumerate(results.right_hand_landmarks.landmark):\n",
    "#             right_hand_joints.append((id, lm.x, lm.y, lm.z))  # Store the landmark ID and normalized coordinates\n",
    "#         hand_joints.append(right_hand_joints)\n",
    "#         #print(\"Right Hand Joints:\", right_hand_joints)\n",
    "\n",
    "#     # Extracting left hand landmarks if available\n",
    "#     if results.left_hand_landmarks:\n",
    "#         left_hand_joints = []\n",
    "#         for id, lm in enumerate(results.left_hand_landmarks.landmark):\n",
    "#             left_hand_joints.append((id, lm.x, lm.y, lm.z))  # Store the landmark ID and normalized coordinates\n",
    "#         hand_joints.append(left_hand_joints)\n",
    "#         print(\"Left Hand Joints:\", left_hand_joints)\n",
    "    \n",
    "#     # Enter key 'q' to break the loop\n",
    "#     if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "#         break\n",
    "\n",
    "# # When all the process is done\n",
    "# # Release the capture\n",
    "# capture.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880c0e4a-f435-4af4-ade4-213ce2c55f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from groq import Groq\n",
    "\n",
    "#     content = str(hand_joints) + \"\\n\\neach of these points is a number followed by a 3 dimensional vector in space (number, x, y, z), each of the numbers corresponds to a joint on the hand. \\n\\n0. WRIST 1. THUMB_CMC 2. THUMB_MCP  3. THUMB_IP 4. THUMB_TIP 5. INDEX FINGER_MCP 6. INDEX FINGER PIP 7. INDEX FINGER_DIP 8. INDEX FINGER TIP 9. MIDDLE FINGER_MCP 10. MIDDLE FINGER_PIP 11. MIDDLE FINGER DIP 12. MIDDLE FINGER TIP 13. RING FINGER MCP 14. RING FINGER PIP 15. RING FINGER_DIP 16. RING FINGER TIP 17. PINKY MCP 18. PINKY PIP 19. PINKY DIP 20. PINKY TIP\\n\\nwhat ASL letter does this create. Please return just the letter.\"\n",
    "\n",
    "#     GROQ_API_KEY = \"\"\n",
    "#     client = Groq(api_key=GROQ_API_KEY)\n",
    "#     completion = client.chat.completions.create(\n",
    "#         model=\"llama-3.2-90b-vision-preview\",\n",
    "#         messages=[\n",
    "#             {\n",
    "#                 \"role\": \"user\",\n",
    "#                 \"content\": content\n",
    "#             }\n",
    "#         ],\n",
    "#         temperature=1,\n",
    "#         max_tokens=1024,\n",
    "#         top_p=1,\n",
    "#         stream=False,\n",
    "#         stop=None,\n",
    "#     )\n",
    "#     print(completion.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b224b585-6d68-4738-91da-27281a9ee900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # (0) in VideoCapture is used to connect to your computer's default camera\n",
    "# capture = cv2.VideoCapture(0)\n",
    "\n",
    "# # Initializing current time and precious time for calculating the FPS\n",
    "# previousTime = 0\n",
    "# currentTime = 0\n",
    "\n",
    "# while capture.isOpened():\n",
    "# \t# capture frame by frame\n",
    "# \tret, frame = capture.read()\n",
    "\n",
    "# \t# resizing the frame for better view\n",
    "# \tframe = cv2.resize(frame, (800, 600))\n",
    "\n",
    "# \t# Converting the from BGR to RGB\n",
    "# \timage = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# \t# Making predictions using holistic model\n",
    "# \t# To improve performance, optionally mark the image as not writeable to\n",
    "# \t# pass by reference.\n",
    "# \timage.flags.writeable = False\n",
    "# \tresults = holistic_model.process(image)\n",
    "# \timage.flags.writeable = True\n",
    "\n",
    "# \t# Converting back the RGB image to BGR\n",
    "# \timage = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# \t# Drawing the Facial Landmarks\n",
    "# \tmp_drawing.draw_landmarks(\n",
    "# \timage,\n",
    "# \tresults.face_landmarks,\n",
    "# \tmp_holistic.FACEMESH_CONTOURS,\n",
    "# \tmp_drawing.DrawingSpec(\n",
    "# \t\tcolor=(255,0,255),\n",
    "# \t\tthickness=1,\n",
    "# \t\tcircle_radius=1\n",
    "# \t),\n",
    "# \tmp_drawing.DrawingSpec(\n",
    "# \t\tcolor=(0,255,255),\n",
    "# \t\tthickness=1,\n",
    "# \t\tcircle_radius=1\n",
    "# \t)\n",
    "# \t)\n",
    "\n",
    "# \t# Drawing Right hand Land Marks\n",
    "# \tmp_drawing.draw_landmarks(\n",
    "# \timage, \n",
    "# \tresults.right_hand_landmarks, \n",
    "# \tmp_holistic.HAND_CONNECTIONS\n",
    "# \t)\n",
    "\n",
    "# \t# Drawing Left hand Land Marks\n",
    "# \tmp_drawing.draw_landmarks(\n",
    "# \timage, \n",
    "# \tresults.left_hand_landmarks, \n",
    "# \tmp_holistic.HAND_CONNECTIONS\n",
    "# \t)\n",
    "\t\n",
    "# \t# Calculating the FPS\n",
    "# \tcurrentTime = time.time()\n",
    "# \tfps = 1 / (currentTime-previousTime)\n",
    "# \tpreviousTime = currentTime\n",
    "\t\n",
    "# \t# Displaying FPS on the image\n",
    "# \tcv2.putText(image, str(int(fps))+\" FPS\", (10, 70), cv2.FONT_HERSHEY_COMPLEX, 1, (0,255,0), 2)\n",
    "\n",
    "# \t# Display the resulting image\n",
    "# \tcv2.imshow(\"Facial and Hand Landmarks\", image)\n",
    "\n",
    "# \t# Enter key 'q' to break the loop\n",
    "# \tif cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "# \t\tbreak\n",
    "\n",
    "# # When all the process is done\n",
    "# # Release the capture and destroy all windows\n",
    "# capture.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6406134-e4b5-4ec2-baba-a9a2524baf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # with different colors\n",
    "# import cv2\n",
    "# import mediapipe as mp\n",
    "# import time\n",
    "# import numpy as np\n",
    "\n",
    "# # Initializing mediapipe modules\n",
    "# mp_holistic = mp.solutions.holistic\n",
    "# mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# # Initialize holistic model\n",
    "# holistic_model = mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# # (0) in VideoCapture is used to connect to your computer's default camera\n",
    "# capture = cv2.VideoCapture(0)\n",
    "\n",
    "# # Initializing current time and previous time for calculating the FPS\n",
    "# previousTime = 0\n",
    "\n",
    "# # Define a list of 21 different colors for each hand landmark\n",
    "# colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0), (255, 0, 255), \n",
    "#           (0, 255, 255), (128, 0, 0), (0, 128, 0), (0, 0, 128), (128, 128, 0),\n",
    "#           (128, 0, 128), (0, 128, 128), (255, 128, 0), (255, 0, 128), (128, 255, 0),\n",
    "#           (0, 255, 128), (128, 0, 255), (0, 128, 255), (255, 128, 128), (128, 255, 128),\n",
    "#           (128, 128, 255)]\n",
    "\n",
    "# while capture.isOpened():\n",
    "#     # Capture frame by frame\n",
    "#     ret, frame = capture.read()\n",
    "\n",
    "#     # Resizing the frame for better view\n",
    "#     frame = cv2.resize(frame, (800, 600))\n",
    "\n",
    "#     # Converting the from BGR to RGB\n",
    "#     image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "#     # Making predictions using holistic model\n",
    "#     image.flags.writeable = False\n",
    "#     results = holistic_model.process(image)\n",
    "#     image.flags.writeable = True\n",
    "\n",
    "#     # Converting back the RGB image to BGR\n",
    "#     image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "#     # If landmarks are detected on the right hand\n",
    "#     if results.right_hand_landmarks:\n",
    "#         # Loop through each landmark point\n",
    "#         for idx, landmark in enumerate(results.right_hand_landmarks.landmark):\n",
    "#             # Get landmark coordinates\n",
    "#             h, w, c = image.shape\n",
    "#             cx, cy = int(landmark.x * w), int(landmark.y * h)\n",
    "\n",
    "#             # Draw circle for each landmark with a different color\n",
    "#             cv2.circle(image, (cx, cy), 7, colors[idx], cv2.FILLED)\n",
    "\n",
    "#         # Optionally, draw hand connections (in a single color if desired)\n",
    "#         mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "\n",
    "#     # Similarly, you can do this for the left hand\n",
    "#     if results.left_hand_landmarks:\n",
    "#         for idx, landmark in enumerate(results.left_hand_landmarks.landmark):\n",
    "#             h, w, c = image.shape\n",
    "#             cx, cy = int(landmark.x * w), int(landmark.y * h)\n",
    "#             cv2.circle(image, (cx, cy), 7, colors[idx], cv2.FILLED)\n",
    "\n",
    "#         mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "\n",
    "#     # Calculating the FPS\n",
    "#     currentTime = time.time()\n",
    "#     fps = 1 / (currentTime - previousTime)\n",
    "#     previousTime = currentTime\n",
    "\n",
    "#     # Displaying FPS on the image\n",
    "#     cv2.putText(image, str(int(fps)) + \" FPS\", (10, 70), cv2.FONT_HERSHEY_COMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "#     # Display the resulting image\n",
    "#     cv2.imshow(\"Hand Landmarks with Different Colors\", image)\n",
    "\n",
    "#     # Enter key 'q' to break the loop\n",
    "#     if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "#         break\n",
    "\n",
    "# # Release the capture and destroy all windows\n",
    "# capture.release()\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6854e3f-8395-4aaa-ac8c-8ca8a9e57bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # hands with numbers\n",
    "# import cv2\n",
    "# import time\n",
    "# import mediapipe as mp\n",
    "\n",
    "# # Initialize MediaPipe components\n",
    "# mp_holistic = mp.solutions.holistic\n",
    "# mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# # Create a holistic model instance\n",
    "# holistic_model = mp_holistic.Holistic()\n",
    "\n",
    "# # (0) in VideoCapture is used to connect to your computer's default camera\n",
    "# capture = cv2.VideoCapture(0)\n",
    "\n",
    "# # Initializing current time and previous time for calculating the FPS\n",
    "# previousTime = 0\n",
    "# currentTime = 0\n",
    "\n",
    "# while capture.isOpened():\n",
    "#     # Capture frame by frame\n",
    "#     ret, frame = capture.read()\n",
    "\n",
    "#     # Resizing the frame for better view\n",
    "#     frame = cv2.resize(frame, (800, 600))\n",
    "\n",
    "#     # Converting the frame from BGR to RGB\n",
    "#     image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "#     # Making predictions using holistic model\n",
    "#     image.flags.writeable = False\n",
    "#     results = holistic_model.process(image)\n",
    "#     image.flags.writeable = True\n",
    "\n",
    "#     # Converting back the RGB image to BGR\n",
    "#     image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "#     # Drawing Right hand Landmarks\n",
    "#     if results.right_hand_landmarks:\n",
    "#         mp_drawing.draw_landmarks(\n",
    "#             image,\n",
    "#             results.right_hand_landmarks,\n",
    "#             mp_holistic.HAND_CONNECTIONS\n",
    "#         )\n",
    "#         # Adding landmark numbers to right hand\n",
    "#         for id, lm in enumerate(results.right_hand_landmarks.landmark):\n",
    "#             h, w, _ = image.shape\n",
    "#             cx, cy = int(lm.x * w), int(lm.y * h)\n",
    "#             cv2.circle(image, (cx, cy), 5, (0, 0, 255), -1)  # Red circle for landmarks\n",
    "#             cv2.putText(image, str(id), (cx, cy - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)  # Label each landmark\n",
    "\n",
    "#     # Drawing Left hand Landmarks\n",
    "#     if results.left_hand_landmarks:\n",
    "#         mp_drawing.draw_landmarks(\n",
    "#             image,\n",
    "#             results.left_hand_landmarks,\n",
    "#             mp_holistic.HAND_CONNECTIONS\n",
    "#         )\n",
    "#         # Adding landmark numbers to left hand\n",
    "#         for id, lm in enumerate(results.left_hand_landmarks.landmark):\n",
    "#             h, w, _ = image.shape\n",
    "#             cx, cy = int(lm.x * w), int(lm.y * h)\n",
    "#             cv2.circle(image, (cx, cy), 5, (0, 0, 255), -1)  # Red circle for landmarks\n",
    "#             cv2.putText(image, str(id), (cx, cy - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255, 255, 255), 1)  # Label each landmark\n",
    "\n",
    "#     # Calculating the FPS\n",
    "#     currentTime = time.time()\n",
    "#     fps = 1 / (currentTime - previousTime)\n",
    "#     previousTime = currentTime\n",
    "\n",
    "#     # Displaying FPS on the image\n",
    "#     cv2.putText(image, str(int(fps)) + \" FPS\", (10, 70), cv2.FONT_HERSHEY_COMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "#     # Display the resulting image\n",
    "#     cv2.imshow(\"Facial and Hand Landmarks\", image)\n",
    "\n",
    "#     # Enter key 'q' to break the loop\n",
    "#     if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "#         break\n",
    "\n",
    "# # When all the process is done\n",
    "# # Release the capture and destroy all windows\n",
    "# capture.release()\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0638990b-7690-4047-aad0-c1f1742852f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to access landmarks\n",
    "for landmark in mp_holistic.HandLandmark:\n",
    "    print(landmark, landmark.value)\n",
    " \n",
    "print(mp_holistic.HandLandmark.WRIST.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e44113e-7413-458f-964f-819251706baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## STEP 1: Import the necessary modules.\n",
    "# import mediapipe as mp\n",
    "# from mediapipe.tasks import python\n",
    "# from mediapipe.tasks.python import vision\n",
    "\n",
    "# STEP 2: Create an HandLandmarker object.\n",
    "# base_options = python.BaseOptions(model_asset_path='hand_landmarker.task')\n",
    "# options = vision.HandLandmarkerOptions(base_options=base_options,\n",
    "#                                        num_hands=2)\n",
    "# detector = vision.HandLandmarker.create_from_options(options)\n",
    "# STEP 3: Load the input image.\n",
    "# image = mp.Image.create_from_file(\"a1.jpg\")\n",
    "\n",
    "# STEP 4: Detect hand landmarks from the input image.\n",
    "# detection_result = detector.detect(image)\n",
    "\n",
    "# STEP 5: Process the classification result. In this case, visualize it.\n",
    "# annotated_image = draw_landmarks_on_image(image.numpy_view(), detection_result)\n",
    "# cv2_imshow(cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR))##"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
